{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca9b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary: \n",
    "dict_df = pd.read_csv('data/loughran_mcdonald_word_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = dict_df[(dict_df[['Negative','Positive', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']] != 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chang_to_ones(col): \n",
    "    if col != 0: \n",
    "        col = 1\n",
    "    return col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9cfe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Negative']= df.Negative.apply(lambda x: chang_to_ones(x))\n",
    "df['Positive']= df.Positive.apply(lambda x: chang_to_ones(x))\n",
    "df['Uncertainty']= df.Uncertainty.apply(lambda x: chang_to_ones(x))\n",
    "df['Litigious']= df.Litigious.apply(lambda x: chang_to_ones(x))\n",
    "df['Strong_Modal']= df.Strong_Modal.apply(lambda x: chang_to_ones(x))\n",
    "df['Weak_Modal']= df.Weak_Modal.apply(lambda x: chang_to_ones(x))\n",
    "df['Constraining']= df.Constraining.apply(lambda x: chang_to_ones(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r') as file:\n",
    "    tester = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d073b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dictionary for each category: \n",
    "def create_dict(df, col): \n",
    "    d = list()\n",
    "    # keep only relevant cols\n",
    "    df= df[['Word', col]]\n",
    "    for index, row in df.iterrows(): \n",
    "        if row[col]==1:\n",
    "            d.append(row['Word'])   \n",
    "        else:\n",
    "            continue\n",
    "    return d   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae029ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Negative','Positive', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']]\n",
    "\n",
    "# create a list of words for each of the categories: \n",
    "\n",
    "# negative dictionary: \n",
    "negative_dict = create_dict(df, 'Negative') # 2355\n",
    "# Positive dictionary: \n",
    "positive_dictionary = create_dict(df, 'Positive')#354\n",
    "# len(positive_dictionary)\n",
    "# uncertainity dictionary: \n",
    "uncertain_dict = create_dict(df, 'Uncertainty')# 297 \n",
    "# len(uncertain_dict) \n",
    "#Litigious dictionary: \n",
    "litigious_dict = create_dict(df, 'Litigious')# 905 \n",
    "# len(litigious_dict) \n",
    "# strong modal dicitonary: \n",
    "strong_modal_dict = create_dict(df, 'Strong_Modal')# 19 \n",
    "# len(strong_modal_dict) \n",
    "# weak modal dicitonary: \n",
    "weak_modal_dict = create_dict(df, 'Weak_Modal')# 27\n",
    "# len(weak_modal_dict)\n",
    "# constraining dictionary \n",
    "constraining_dict = create_dict(df, 'Constraining')# 184\n",
    "len(constraining_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(acc_nr, text): \n",
    "    text \n",
    "    negative_cnt = 0 \n",
    "    positive_cnt = 0 \n",
    "    uncertain_cnt = 0 \n",
    "    litigious_cnt = 0\n",
    "    strong_m_cnt = 0\n",
    "    weak_m_cnt = 0 \n",
    "    constraining_cnt = 0 \n",
    "    \n",
    "    # DICTIONARY of negative word counts: \n",
    "    negative_word_dict = dict()\n",
    "    negative_word_prop= dict()\n",
    "    \n",
    "    positive_word_dict= dict()\n",
    "    positive_word_prop= dict()\n",
    "    \n",
    "    uncertain_word_dict = dict()\n",
    "    uncertain_word_prop = dict()\n",
    "    litigious_word_dict = dict()\n",
    "    litigious_word_prop= dict()\n",
    "    strong_mod_word_dict = dict()\n",
    "    strong_mod_word_prop= dict()\n",
    "    weak_mod_word_dict= dict()\n",
    "    weak_mod_word_prop= dict()\n",
    "    constraining_word_dict= dict()\n",
    "    constraining_word_prop= dict()    \n",
    "    weight_dict = dict()\n",
    "    \n",
    "    doc_lst = nltk.wordpunct_tokenize(text.upper()) #list of all words in the document\n",
    "    clean_list = list()\n",
    "    \n",
    "    # clean the document and remove unncecesary letters and numbers: \n",
    "    for item in doc_lst: \n",
    "        if item.isalpha() and ((len(item)>1) or item == 'A'): \n",
    "            clean_list.append(item)\n",
    "    \n",
    "    \n",
    "    doc_len = len(clean_list) # length of the clean document \n",
    "    for item in clean_list:      \n",
    "            if item in negative_dict: \n",
    "                negative_cnt+=1 # gives total negative cnt in the documents(not removing duplicates)\n",
    "                #             if item not in negative.i: \n",
    "                if item not in negative_word_dict:\n",
    "                    negative_word_dict[item] = clean_list.count(item)\n",
    "                    negative_word_prop[item]= (negative_word_dict[item])/(doc_len)\n",
    "                \n",
    "            if item in positive_dictionary: \n",
    "                positive_cnt+=1\n",
    "                if item not in positive_word_dict:\n",
    "                    positive_word_dict[item] = clean_list.count(item)\n",
    "                    positive_word_prop[item]= (positive_word_dict[item])/(doc_len)\n",
    "                \n",
    "            if item in uncertain_dict: \n",
    "                uncertain_cnt+=1\n",
    "                if item not in uncertain_word_dict:\n",
    "                    uncertain_word_dict[item] = clean_list.count(item)\n",
    "                    uncertain_word_prop[item]= (uncertain_word_dict[item])/(doc_len)    \n",
    "                \n",
    "            if item in litigious_dict: \n",
    "                litigious_cnt+=1\n",
    "                if item not in litigious_word_dict:\n",
    "                    litigious_word_dict[item] = clean_list.count(item)\n",
    "                    litigious_word_prop[item]= (litigious_word_dict[item])/(doc_len)\n",
    "                \n",
    "                \n",
    "            if item in strong_modal_dict: \n",
    "                strong_m_cnt+=1\n",
    "                if item not in strong_mod_word_dict:\n",
    "                    strong_mod_word_dict[item] = clean_list.count(item)\n",
    "                    strong_mod_word_prop[item]= (strong_mod_word_dict[item])/(doc_len)\n",
    "                \n",
    "            if item in weak_modal_dict: \n",
    "                weak_m_cnt+=1\n",
    "                if item not in weak_mod_word_dict:\n",
    "                    weak_mod_word_dict[item] = clean_list.count(item)\n",
    "                    weak_mod_word_prop[item]= (weak_mod_word_dict[item])/(doc_len)\n",
    "                \n",
    "            if item in constraining_dict: \n",
    "                constraining_cnt+=1\n",
    "                if item not in constraining_word_dict:\n",
    "                    constraining_word_dict[item] = clean_list.count(item)\n",
    "                    constraining_word_prop[item]= (constraining_word_dict[item])/(doc_len)\n",
    "                \n",
    "                \n",
    "            else: \n",
    "                continue \n",
    "#       'acc_nr': acc_nr  \n",
    "    weight_dict= {  'acc_nr': acc_nr, 'negative': negative_cnt, 'positive': positive_cnt, 'uncertain': uncertain_cnt,\n",
    "                  'litigious':litigious_cnt, 'strong_m': strong_m_cnt, 'weak_m':weak_m_cnt,\n",
    "                  'constraining': constraining_cnt, 'negative_freq':negative_word_dict, 'negative_prop':negative_word_prop,\n",
    "                 'positive_freq':positive_word_dict, 'positive_prop': positive_word_prop, \n",
    "                  'uncertain_freq':uncertain_word_dict, 'uncertain_prop':uncertain_word_prop, \n",
    "                 'litigious_freq':litigious_word_dict, 'litigious_prop':litigious_word_prop, \n",
    "                 'strong_mod_freq':strong_mod_word_dict, 'strong_mod_prop':strong_mod_word_prop, \n",
    "                 'weak_mod_freq':weak_mod_word_dict, 'weak_mod_prop':weak_mod_word_prop, \n",
    "                 'constraining_freq':constraining_word_dict, 'constraining_prop':constraining_word_prop }\n",
    "    \n",
    "    return weight_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d613808-62a0-4351-9364-69ddf5d0f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of old acc numbers so we don't have to run it through the json scrape again\n",
    "acc_nr_store = json.load(open('metastore/master_output_2', 'r'))\n",
    "old_acc_nr = []\n",
    "for i in range(len(acc_nr_store)):\n",
    "    old_acc_nr.append(acc_nr_store[i]['acc_nr'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30020e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_nr_store = json.load(open(\"metastore/acc_nr_store.json\", \"r\"))\n",
    "def load_sec_filing(acc_nr, path):\n",
    "    with open(os.path.join(path, f'{acc_nr}.html'), 'r') as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')#lxml\n",
    "    return soup\n",
    "    \n",
    "def clean_filing(soup):\n",
    "    # remove all table tags from document\n",
    "    for s in soup.select(\"table\"):\n",
    "        s.extract()\n",
    "        \n",
    "    # only keep document tag (removes metadata)\n",
    "    for s in soup.select(\"document\"):\n",
    "        document = s\n",
    "        break\n",
    "    document = document.get_text()\n",
    "    document = re.sub(r\"[\\n|\\s]\", \" \", document)\n",
    "    document = re.sub(r\"[^\\s|\\w|\\d]\", \" \", document)\n",
    "    return document\n",
    "\n",
    "\n",
    "bag_of_words_store = list()\n",
    "for cik in acc_nr_store:\n",
    "    # sentiment_store[cik] = {}\n",
    "    for acc_nr in tqdm(acc_nr_store[cik]):\n",
    "            if acc_nr not in old_acc_nr: \n",
    "                soup = load_sec_filing(acc_nr, \"metastore/filings\")\n",
    "                document = clean_filing(soup)\n",
    "                bag_of_words_store.append(get_weight(acc_nr, document))\n",
    "            else: \n",
    "                continue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d39985",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metastore/master_output_3', 'w') as fout:\n",
    "    json.dump(bag_of_words_store, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19fca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015-2021 data\n",
    "import json\n",
    "\n",
    "with open('metastore/master_output_2') as f:\n",
    "    raw_master_output = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13767c68-e85f-48a2-ae55-3e3d554b12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2001-2015 data\n",
    "import json\n",
    "\n",
    "with open('metastore/master_output_3') as f:\n",
    "    raw_master_output = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3292a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate word count of 10-K, total proportion of negative words, expected average negative ###\n",
    "### This calculation is performed for each 10-K filing\n",
    "for i in range(len(raw_master_output)):\n",
    "    \n",
    "    if len(list(raw_master_output[i]['negative_freq'].keys())) != 0:\n",
    "        first_key_negative_freq = list(raw_master_output[i]['negative_freq'].keys())[0]\n",
    "        first_key_negative_prop = list(raw_master_output[i]['negative_prop'].keys())[0]\n",
    "        \n",
    "        doc_len_individual_10k = raw_master_output[i]['negative_freq'][first_key_negative_freq] / raw_master_output[i]['negative_prop'][first_key_negative_prop]\n",
    "        raw_master_output[i]['doc_len'] = round(doc_len_individual_10k)\n",
    "        \n",
    "    elif len(list(raw_master_output[i]['positive_freq'].keys())) != 0:\n",
    "        first_key_positive_freq = list(raw_master_output[i]['positive_freq'].keys())[0]\n",
    "        first_key_positive_prop = list(raw_master_output[i]['positive_prop'].keys())[0]\n",
    "        \n",
    "        doc_len_individual_10k = raw_master_output[i]['positive_freq'][ first_key_positive_freq] / raw_master_output[i]['positive_prop'][first_key_positive_prop]\n",
    "        raw_master_output[i]['doc_len'] = round(doc_len_individual_10k)\n",
    "        \n",
    "    elif len(list(raw_master_output[i]['uncertain_freq'].keys())) != 0:\n",
    "        first_key_uncertain_freq = list(raw_master_output[i]['uncertain_freq'].keys())[0]\n",
    "        first_key_uncertain_prop = list(raw_master_output[i]['uncertain_prop'].keys())[0]\n",
    "        \n",
    "        doc_len_individual_10k = raw_master_output[i]['uncertain_freq'][first_key_uncertain_fre] / raw_master_output[i]['uncertain_prop'][first_key_uncertain_prop]\n",
    "        raw_master_output[i]['doc_len'] = round(doc_len_individual_10k)\n",
    "    \n",
    "    elif len(list(raw_master_output[i]['litigious_freq'].keys())) != 0:\n",
    "        first_key_litigious_freq = list(raw_master_output[i]['litigious_freq'].keys())[0]\n",
    "        first_key_litigious_prop = list(raw_master_output[i]['litigious_prop'].keys())[0]\n",
    "        \n",
    "        doc_len_individual_10k = raw_master_output[i]['litigious_freq'][first_key_litigious_freq] / raw_master_output[i]['litigious_prop'][first_key_litigious_prop]\n",
    "        raw_master_output[i]['doc_len'] = round(doc_len_individual_10k)\n",
    "        \n",
    "    elif len(list(raw_master_output[i]['strong_mod_freq'].keys())) != 0:\n",
    "        first_key_strong_m_freq = list(raw_master_output[i]['strong_mod_freq'].keys())[0]\n",
    "        first_key_strong_m_prop = list(raw_master_output[i]['strong_mod_prop'].keys())[0]\n",
    "        \n",
    "        doc_len_individual_10k = raw_master_output[i]['strong_mod_freq'][first_key_strong_m_freq] / raw_master_output[i]['strong_mod_prop'][first_key_strong_m_prop]\n",
    "        raw_master_output[i]['doc_len'] = round(doc_len_individual_10k)\n",
    "        \n",
    "    elif len(list(raw_master_output[i]['weak_mod_freq'].keys())) != 0:\n",
    "        first_key_weak_m_freq = list(raw_master_output[i]['weak_mod_freq'].keys())[0]\n",
    "        first_key_weak_m_prop = list(raw_master_output[i]['weak_mod_prop'].keys())[0]\n",
    "        \n",
    "        doc_len_individual_10k = raw_master_output[i]['weak_mod_freq'][first_key_weak_m_freq] / raw_master_output[i]['weak_mod_prop'][first_key_weak_m_prop]\n",
    "        raw_master_output[i]['doc_len'] = round(doc_len_individual_10k)\n",
    "        \n",
    "    elif len(list(raw_master_output[i]['constraining_freq'].keys())) != 0:\n",
    "        first_key_constraining_freq = list(raw_master_output[i]['constraining_freq'].keys())[0]\n",
    "        first_key_constraining_prop = list(raw_master_output[i]['constraining_prop'].keys())[0]\n",
    "        \n",
    "        doc_len_individual_10k = raw_master_output[i]['constraining_freq'][first_key_constraining_freq] / raw_master_output[i]['constraining_prop'][first_key_constraining_prop]\n",
    "        raw_master_output[i]['doc_len'] = round(doc_len_individual_10k)\n",
    "    \n",
    "    else:\n",
    "        raw_master_output[i]['doc_len'] = 0\n",
    "    \n",
    "    # Calculation of total proportion of each word label\n",
    "    # Negative\n",
    "    if raw_master_output[i]['negative'] == 0:\n",
    "        raw_master_output[i]['total_negative_proportion'] = 0\n",
    "    if raw_master_output[i]['negative'] != 0:\n",
    "        raw_master_output[i]['total_negative_proportion'] = raw_master_output[i]['negative'] / raw_master_output[i]['doc_len']\n",
    "        \n",
    "    # Positive\n",
    "    if raw_master_output[i]['positive'] == 0:\n",
    "        raw_master_output[i]['total_positive_proportion'] = 0\n",
    "    if  raw_master_output[i]['positive'] != 0:\n",
    "        raw_master_output[i]['total_positive_proportion'] = raw_master_output[i]['positive'] / raw_master_output[i]['doc_len']\n",
    "        \n",
    "    # Uncertain       \n",
    "    if raw_master_output[i]['uncertain'] == 0:\n",
    "        raw_master_output[i]['total_uncertain_proportion'] = 0\n",
    "    if raw_master_output[i]['uncertain'] != 0:\n",
    "        raw_master_output[i]['total_uncertain_proportion'] = raw_master_output[i]['uncertain'] / raw_master_output[i]['doc_len']\n",
    "    \n",
    "    # Litigious\n",
    "    if raw_master_output[i]['litigious'] == 0:\n",
    "        raw_master_output[i]['total_litigious_proportion'] = 0\n",
    "    if raw_master_output[i]['litigious'] != 0:\n",
    "        raw_master_output[i]['total_litigious_proportion'] = raw_master_output[i]['litigious'] / raw_master_output[i]['doc_len']\n",
    "    \n",
    "    # Strong_m\n",
    "    if raw_master_output[i]['strong_m'] == 0:\n",
    "        raw_master_output[i]['total_strong_m_proportion'] = 0\n",
    "    if raw_master_output[i]['strong_m'] != 0:\n",
    "        raw_master_output[i]['total_strong_m_proportion'] = raw_master_output[i]['strong_m'] / raw_master_output[i]['doc_len']\n",
    "    \n",
    "    # Weak_m\n",
    "    if raw_master_output[i]['weak_m'] == 0:\n",
    "        raw_master_output[i]['total_weak_m_proportion'] = 0\n",
    "    if raw_master_output[i]['weak_m'] != 0:\n",
    "        raw_master_output[i]['total_weak_m_proportion'] = raw_master_output[i]['weak_m'] / raw_master_output[i]['doc_len']\n",
    "    \n",
    "    # Constraining\n",
    "    if raw_master_output[i]['constraining'] == 0:\n",
    "         raw_master_output[i]['total_constraining_proportion'] = 0\n",
    "    if raw_master_output[i]['constraining'] != 0:\n",
    "        raw_master_output[i]['total_constraining_proportion'] = raw_master_output[i]['constraining'] / raw_master_output[i]['doc_len']\n",
    "    \n",
    "    # Calculation of average value for each word label\n",
    "    # Negative\n",
    "    if len(list(raw_master_output[i]['negative_freq'].keys())) != 0:\n",
    "        expected_average_negative = raw_master_output[i]['negative'] / len(list(raw_master_output[i]['negative_freq'].keys()))\n",
    "        raw_master_output[i]['expected_average_negative'] = expected_average_negative\n",
    "    else:\n",
    "        raw_master_output[i]['expected_average_negative'] = 0\n",
    "    \n",
    "    # Positive\n",
    "    if len(list(raw_master_output[i]['positive_freq'].keys())) != 0:\n",
    "        expected_average_positive = raw_master_output[i]['positive'] / len(list(raw_master_output[i]['positive_freq'].keys()))\n",
    "        raw_master_output[i]['expected_average_positive'] = expected_average_positive\n",
    "    else: \n",
    "        raw_master_output[i]['expected_average_positive'] = 0\n",
    "        \n",
    "    # Uncertain\n",
    "    if len(list(raw_master_output[i]['uncertain_freq'].keys())) != 0:\n",
    "        expected_average_uncertain = raw_master_output[i]['uncertain'] / len(list(raw_master_output[i]['uncertain_freq'].keys()))\n",
    "        raw_master_output[i]['expected_average_uncertain'] = expected_average_uncertain\n",
    "    else:\n",
    "        raw_master_output[i]['expected_average_uncertain'] = 0\n",
    "    \n",
    "    # Litigious\n",
    "    if len(list(raw_master_output[i]['litigious_freq'].keys())) != 0:\n",
    "        expected_average_litigious = raw_master_output[i]['litigious'] / len(list(raw_master_output[i]['litigious_freq'].keys()))\n",
    "        raw_master_output[i]['expected_average_litigious'] = expected_average_litigious\n",
    "    else: \n",
    "        raw_master_output[i]['expected_average_litigious'] = 0\n",
    "        \n",
    "    # Strong_m\n",
    "    if len(list(raw_master_output[i]['strong_mod_freq'].keys())) != 0:\n",
    "        expected_average_strong_m = raw_master_output[i]['strong_m'] / len(list(raw_master_output[i]['strong_mod_freq'].keys()))\n",
    "        raw_master_output[i]['expected_average_strong_m'] = expected_average_strong_m\n",
    "    else:\n",
    "        raw_master_output[i]['expected_average_strong_m'] = 0\n",
    "    \n",
    "    # Weak_m\n",
    "    if len(list(raw_master_output[i]['weak_mod_freq'].keys())) != 0:\n",
    "        expected_average_weak_m = raw_master_output[i]['weak_m'] / len(list(raw_master_output[i]['weak_mod_freq'].keys()))\n",
    "        raw_master_output[i]['expected_average_weak_m'] = expected_average_weak_m\n",
    "    else: \n",
    "        raw_master_output[i]['expected_average_weak_m'] = 0\n",
    "    \n",
    "    # Constraining\n",
    "    if len(list(raw_master_output[i]['constraining_freq'].keys())) != 0:\n",
    "        expected_average_constraining = raw_master_output[i]['constraining'] / len(list(raw_master_output[i]['constraining_freq'].keys()))\n",
    "        raw_master_output[i]['expected_average_constraining'] = expected_average_constraining\n",
    "    else:\n",
    "        raw_master_output[i]['expected_average_constraining'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed916ab0-a94a-497e-aee8-fd54bdcaacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original 2001-2015 data had 1318 10-K's, after filtering we came to 1291 10-K's\n",
    "master_output = []\n",
    "for i in range(len(raw_master_output)):\n",
    "    if raw_master_output[i]['doc_len'] > 2000:\n",
    "        master_output.append(raw_master_output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Calculate number of occurrence of each word in all the 10-K filings ###\n",
    "negative_word_list = list()\n",
    "positive_word_list = list()\n",
    "uncertain_word_list = list()\n",
    "litigious_word_list = list()\n",
    "strong_m_word_list = list()\n",
    "weak_m_word_list= list()\n",
    "constraining_word_list= list()\n",
    "\n",
    "\n",
    "for i in range(len(master_output)): \n",
    "    temp = list(master_output[i]['negative_freq'].keys())\n",
    "    negative_word_list.extend(temp) # contains all negative words in all 10k filings\n",
    "    \n",
    "    positive_temp = list(master_output[i]['positive_freq'].keys())\n",
    "    positive_word_list.extend(positive_temp)\n",
    "    \n",
    "    # uncertain: \n",
    "    uncertain_temp = list(master_output[i]['uncertain_freq'].keys())\n",
    "    uncertain_word_list.extend(uncertain_temp)\n",
    "    \n",
    "    #litigious: \n",
    "    litigious_temp = list(master_output[i]['litigious_freq'].keys())\n",
    "    litigious_word_list.extend(litigious_temp)\n",
    "    \n",
    "    #strong m: \n",
    "    strong_m_temp = list(master_output[i]['strong_mod_freq'].keys())\n",
    "    strong_m_word_list.extend(strong_m_temp)\n",
    "    \n",
    "    #weak m:\n",
    "    weak_m_temp = list(master_output[i]['weak_mod_freq'].keys())\n",
    "    weak_m_word_list.extend(weak_m_temp)\n",
    "    \n",
    "    # constraining: \n",
    "    constraining_temp = list(master_output[i]['constraining_freq'].keys())\n",
    "    constraining_word_list.extend(constraining_temp)\n",
    "        \n",
    "\n",
    "# Create dictionary of counts of each negative word across 10-K filings\n",
    "df_dict = dict()\n",
    "for item in negative_word_list: \n",
    "    if item not in df_dict: \n",
    "        df_dict[item]= negative_word_list.count(item)\n",
    "        \n",
    "        \n",
    "        \n",
    "#POSITIVE: Create dictionary of counts of each negative word across 10-K filings\n",
    "positive_df_dict = dict()\n",
    "for item in positive_word_list: \n",
    "    if item not in positive_df_dict: \n",
    "        positive_df_dict[item]= positive_word_list.count(item)\n",
    "        \n",
    "#uncertain: Create dictionary of counts of each uncertain word across 10-K filings\n",
    "uncertain_df_dict = dict()\n",
    "for item in uncertain_word_list: \n",
    "    if item not in uncertain_df_dict: \n",
    "        uncertain_df_dict[item]= uncertain_word_list.count(item)  \n",
    "\n",
    "#litigious: Create dictionary of counts of each litigious word across 10-K filings\n",
    "litigious_df_dict = dict()\n",
    "for item in litigious_word_list: \n",
    "    if item not in litigious_df_dict: \n",
    "        litigious_df_dict[item]= litigious_word_list.count(item)\n",
    "        \n",
    "#strongm: Create dictionary of counts of each strongm word across 10-K filings\n",
    "strong_m_df_dict = dict()\n",
    "for item in strong_m_word_list: \n",
    "    if item not in strong_m_df_dict: \n",
    "        strong_m_df_dict[item]= strong_m_word_list.count(item)\n",
    "        \n",
    "\n",
    "#weakm: Create dictionary of counts of each weakm word across 10-K filings\n",
    "weak_m_df_dict = dict()\n",
    "for item in weak_m_word_list: \n",
    "    if item not in weak_m_df_dict: \n",
    "        weak_m_df_dict[item]= weak_m_word_list.count(item)     \n",
    "        \n",
    "\n",
    "#constraining: Create dictionary of counts of each weakm word across 10-K filings\n",
    "constraining_df_dict = dict()\n",
    "for item in constraining_word_list: \n",
    "    if item not in constraining_df_dict: \n",
    "        constraining_df_dict[item]= constraining_word_list.count(item) \n",
    "        \n",
    "###########################################################################################################################\n",
    "#1 Added df_dict to each 10-K filing (only added words found in 10-K)\n",
    "for i in range(len(master_output)):\n",
    "    master_output[i]['negative_df_dict'] = dict()\n",
    "    temp = list(master_output[i]['negative_freq'].keys())\n",
    "    temp_df_dict = list(df_dict.keys())\n",
    "    for key in temp_df_dict:\n",
    "        if key in temp:\n",
    "            master_output[i]['negative_df_dict'][key] = df_dict[key]\n",
    "#2. \n",
    "for i in range(len(master_output)):\n",
    "    master_output[i]['positive_df_dict'] = dict()\n",
    "    temp = list(master_output[i]['positive_freq'].keys())\n",
    "    temp_df_dict = list(positive_df_dict.keys())\n",
    "    for key in temp_df_dict:\n",
    "        if key in temp:\n",
    "            master_output[i]['positive_df_dict'][key] = positive_df_dict[key]\n",
    "\n",
    "            \n",
    "#3.uncertain_df_dict\n",
    "for i in range(len(master_output)):\n",
    "    master_output[i]['uncertain_df_dict'] = dict()\n",
    "    temp = list(master_output[i]['uncertain_freq'].keys())\n",
    "    temp_df_dict = list(uncertain_df_dict.keys())\n",
    "    for key in temp_df_dict:\n",
    "        if key in temp:\n",
    "            master_output[i]['uncertain_df_dict'][key] = uncertain_df_dict[key]\n",
    "\n",
    "#4.litigious_df_dict\n",
    "for i in range(len(master_output)):\n",
    "    master_output[i]['litigious_df_dict'] = dict()\n",
    "    temp = list(master_output[i]['litigious_freq'].keys())\n",
    "    temp_df_dict = list(litigious_df_dict.keys())\n",
    "    for key in temp_df_dict:\n",
    "        if key in temp:\n",
    "            master_output[i]['litigious_df_dict'][key] = litigious_df_dict[key]\n",
    "\n",
    "#5.strong_mod_freq\n",
    "for i in range(len(master_output)):\n",
    "    master_output[i]['strong_m_df_dict'] = dict()\n",
    "    temp = list(master_output[i]['strong_mod_freq'].keys())\n",
    "    temp_df_dict = list(strong_m_df_dict.keys())\n",
    "    for key in temp_df_dict:\n",
    "        if key in temp:\n",
    "            master_output[i]['strong_m_df_dict'][key] = strong_m_df_dict[key]\n",
    "#6.weak_mod_freq\n",
    "for i in range(len(master_output)):\n",
    "    master_output[i]['weak_m_df_dict'] = dict()\n",
    "    temp = list(master_output[i]['weak_mod_freq'].keys())\n",
    "    temp_df_dict = list(weak_m_df_dict.keys())\n",
    "    for key in temp_df_dict:\n",
    "        if key in temp:\n",
    "            master_output[i]['weak_m_df_dict'][key] = weak_m_df_dict[key]\n",
    "            \n",
    "#7.constraining_df_dict constraining_freq\n",
    "for i in range(len(master_output)):\n",
    "    master_output[i]['constraining_df_dict'] = dict()\n",
    "    temp = list(master_output[i]['constraining_freq'].keys())\n",
    "    temp_df_dict = list(constraining_df_dict.keys())\n",
    "    for key in temp_df_dict:\n",
    "        if key in temp:\n",
    "            master_output[i]['constraining_df_dict'][key] = constraining_df_dict[key]\n",
    "###########################################################################################################################            \n",
    "# # Reorder negative_freq & df_dict alphabetically in each 10-K filing\n",
    "for i in range(len(master_output)):\n",
    "    master_output[i]['negative_freq'] = dict(sorted(master_output[0]['negative_freq'].items(), key = lambda x: x[0], reverse = False))\n",
    "    master_output[i]['negative_df_dict'] = dict(sorted(master_output[0]['negative_df_dict'].items(), key = lambda x: x[0], reverse = False))\n",
    "    \n",
    "    #positive: \n",
    "    master_output[i]['positive_freq'] = dict(sorted(master_output[0]['positive_freq'].items(), key = lambda x: x[0], reverse = False))\n",
    "    master_output[i]['positive_df_dict'] = dict(sorted(master_output[0]['positive_df_dict'].items(), key = lambda x: x[0], reverse = False))\n",
    "    \n",
    "    # uncertain: \n",
    "    master_output[i]['uncertain_freq'] = dict(sorted(master_output[0]['uncertain_freq'].items(), key = lambda x: x[0], reverse = False))\n",
    "    master_output[i]['uncertain_df_dict'] = dict(sorted(master_output[0]['uncertain_df_dict'].items(), key = lambda x: x[0], reverse = False))\n",
    "    \n",
    "    #litigious: \n",
    "    master_output[i]['litigious_freq'] = dict(sorted(master_output[0]['litigious_freq'].items(), key = lambda x: x[0], reverse = False))\n",
    "    master_output[i]['litigious_df_dict'] = dict(sorted(master_output[0]['litigious_df_dict'].items(), key = lambda x: x[0], reverse = False))\n",
    "    \n",
    "    #strong_mod_freq\n",
    "    \n",
    "    master_output[i]['strong_mod_freq'] = dict(sorted(master_output[0]['strong_mod_freq'].items(), key = lambda x: x[0], reverse = False))\n",
    "    master_output[i]['strong_m_df_dict'] = dict(sorted(master_output[0]['strong_m_df_dict'].items(), key = lambda x: x[0], reverse = False))\n",
    "    \n",
    "    # weak mod:\n",
    "    master_output[i]['weak_mod_freq'] = dict(sorted(master_output[0]['weak_mod_freq'].items(), key = lambda x: x[0], reverse = False))\n",
    "    master_output[i]['weak_m_df_dict'] = dict(sorted(master_output[0]['weak_m_df_dict'].items(), key = lambda x: x[0], reverse = False))\n",
    "    \n",
    "    \n",
    "    # constraining\n",
    "    master_output[i]['constraining_freq'] = dict(sorted(master_output[0]['constraining_freq'].items(), key = lambda x: x[0], reverse = False))\n",
    "    master_output[i]['constraining_df_dict'] = dict(sorted(master_output[0]['constraining_df_dict'].items(), key = lambda x: x[0], reverse = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38caf6b1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "w_i_j =[ ( 1 + log(tf_i_j) ) / ( 1 + log(a_j) ) ] * log(N/df_i)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate weight of each word in each 10-K filing ###\n",
    "\n",
    "N = len(master_output)\n",
    "\n",
    "# for i in range(len(master_output)):\n",
    "#     negative_word_list_per_10k = list(master_output[i]['negative_freq'].keys())\n",
    "#     master_output[i]['weight_dict'] = dict()\n",
    "    \n",
    "#     for word in negative_word_list_per_10k:\n",
    "#         numerator = 1 + np.log(master_output[i]['negative_freq'][word])\n",
    "#         denominator = 1 + np.log(master_output[i]['expected_average_negative'])\n",
    "#         multiplication_term = np.log(N/master_output[i]['df_dict'][word]) # Multiply by log(N/number of times that word shows up across 10-K)\n",
    "        \n",
    "#         master_output[i]['weight_dict'][word] = (numerator / denominator) * multiplication_term\n",
    "#         master_output[i]['summed_negative_weights'] = sum(list(master_output[i]['weight_dict'].values())) # Final weight calculation\n",
    "\n",
    "for i in range(len(master_output)):\n",
    "    \n",
    "    negative_word_list_per_10k = list(master_output[i]['negative_freq'].keys())\n",
    "    master_output[i]['weight_dict_negative'] = dict()\n",
    "    \n",
    "    positive_word_list_per_10k = list(master_output[i]['positive_freq'].keys())\n",
    "    master_output[i]['weight_dict_positive'] = dict()\n",
    "    \n",
    "    uncertain_word_list_per_10k = list(master_output[i]['uncertain_freq'].keys())\n",
    "    master_output[i]['weight_dict_uncertain'] = dict()\n",
    "    \n",
    "    litigious_word_list_per_10k = list(master_output[i]['litigious_freq'].keys())\n",
    "    master_output[i]['weight_dict_litigious'] = dict()\n",
    "    \n",
    "    strong_m_word_list_per_10k = list(master_output[i]['strong_mod_freq'].keys())\n",
    "    master_output[i]['weight_dict_strong_m'] = dict()\n",
    "    \n",
    "    weak_m_word_list_per_10k = list(master_output[i]['weak_mod_freq'].keys())\n",
    "    master_output[i]['weight_dict_weak_m'] = dict()\n",
    "    \n",
    "    constraining_word_list_per_10k = list(master_output[i]['constraining_freq'].keys())\n",
    "    master_output[i]['weight_dict_constraining'] = dict()\n",
    "    \n",
    "    # Negative\n",
    "    for word in negative_word_list_per_10k:\n",
    "        numerator = 1 + np.log(master_output[i]['negative_freq'][word])\n",
    "        denominator = 1 + np.log(master_output[i]['expected_average_negative'])\n",
    "        multiplication_term = np.log(N/master_output[i]['negative_df_dict'][word]) # Multiply by log(N/number of times that word shows up across 10-K)\n",
    "        \n",
    "        master_output[i]['weight_dict_negative'][word] = (numerator / denominator) * multiplication_term\n",
    "        master_output[i]['summed_negative_weights'] = sum(list(master_output[i]['weight_dict_negative'].values())) # Final weight calculation\n",
    "    \n",
    "    # Positive\n",
    "    for word in positive_word_list_per_10k:\n",
    "        numerator = 1 + np.log(master_output[i]['positive_freq'][word])\n",
    "        denominator = 1 + np.log(master_output[i]['expected_average_positive'])\n",
    "        multiplication_term = np.log(N/master_output[i]['positive_df_dict'][word]) # Multiply by log(N/number of times that word shows up across 10-K)\n",
    "        \n",
    "        master_output[i]['weight_dict_positive'][word] = (numerator / denominator) * multiplication_term\n",
    "        master_output[i]['summed_positive_weights'] = sum(list(master_output[i]['weight_dict_positive'].values())) # Final weight calculation\n",
    "\n",
    "    # Uncertain\n",
    "    for word in uncertain_word_list_per_10k:\n",
    "        numerator = 1 + np.log(master_output[i]['uncertain_freq'][word])\n",
    "        denominator = 1 + np.log(master_output[i]['expected_average_uncertain'])\n",
    "        multiplication_term = np.log(N/master_output[i]['uncertain_df_dict'][word]) # Multiply by log(N/number of times that word shows up across 10-K)\n",
    "        \n",
    "        master_output[i]['weight_dict_uncertain'][word] = (numerator / denominator) * multiplication_term\n",
    "        master_output[i]['summed_uncertain_weights'] = sum(list(master_output[i]['weight_dict_uncertain'].values())) # Final weight calculation\n",
    "\n",
    "    # Litigious\n",
    "    for word in litigious_word_list_per_10k:\n",
    "        numerator = 1 + np.log(master_output[i]['litigious_freq'][word])\n",
    "        denominator = 1 + np.log(master_output[i]['expected_average_litigious'])\n",
    "        multiplication_term = np.log(N/master_output[i]['litigious_df_dict'][word]) # Multiply by log(N/number of times that word shows up across 10-K)\n",
    "        \n",
    "        master_output[i]['weight_dict_litigious'][word] = (numerator / denominator) * multiplication_term\n",
    "        master_output[i]['summed_litigious_weights'] = sum(list(master_output[i]['weight_dict_litigious'].values())) # Final weight calculation\n",
    "   \n",
    "    # Strong_m\n",
    "    for word in strong_m_word_list_per_10k:\n",
    "        numerator = 1 + np.log(master_output[i]['strong_mod_freq'][word])\n",
    "        denominator = 1 + np.log(master_output[i]['expected_average_strong_m'])\n",
    "        multiplication_term = np.log(N/master_output[i]['strong_m_df_dict'][word]) # Multiply by log(N/number of times that word shows up across 10-K)\n",
    "        \n",
    "        master_output[i]['weight_dict_strong_m'][word] = (numerator / denominator) * multiplication_term\n",
    "        master_output[i]['summed_strong_m_weights'] = sum(list(master_output[i]['weight_dict_strong_m'].values())) # Final weight calculation\n",
    "\n",
    "    # Weak_m\n",
    "    for word in weak_m_word_list_per_10k:\n",
    "        numerator = 1 + np.log(master_output[i]['weak_mod_freq'][word])\n",
    "        denominator = 1 + np.log(master_output[i]['expected_average_weak_m'])\n",
    "        multiplication_term = np.log(N/master_output[i]['weak_m_df_dict'][word]) # Multiply by log(N/number of times that word shows up across 10-K)\n",
    "        \n",
    "        master_output[i]['weight_dict_weak_m'][word] = (numerator / denominator) * multiplication_term\n",
    "        master_output[i]['summed_weak_m_weights'] = sum(list(master_output[i]['weight_dict_weak_m'].values())) # Final weight calculation\n",
    "\n",
    "    # Constraining\n",
    "    for word in constraining_word_list_per_10k:\n",
    "        numerator = 1 + np.log(master_output[i]['constraining_freq'][word])\n",
    "        denominator = 1 + np.log(master_output[i]['expected_average_negative'])\n",
    "        multiplication_term = np.log(N/master_output[i]['constraining_df_dict'][word]) # Multiply by log(N/number of times that word shows up across 10-K)\n",
    "        \n",
    "        master_output[i]['weight_dict_constraining'][word] = (numerator / denominator) * multiplication_term\n",
    "        master_output[i]['summed_constraining_weights'] = sum(list(master_output[i]['weight_dict_constraining'].values())) # Final weight calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f40823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dictionary\n",
    "\n",
    "final_list = list()\n",
    "for i in range(len(master_output)):\n",
    "    temp = dict()\n",
    "    temp['acc_nr'] = master_output[i]['acc_nr']\n",
    "    temp['total_negative_proportion'] = master_output[i]['total_negative_proportion']\n",
    "    temp['total_positive_proportion'] = master_output[i]['total_positive_proportion']\n",
    "    temp['total_uncertain_proportion'] = master_output[i]['total_uncertain_proportion']\n",
    "    temp['total_litigious_proportion'] = master_output[i]['total_litigious_proportion']\n",
    "    temp['total_strong_m_proportion'] = master_output[i]['total_strong_m_proportion']\n",
    "    temp['total_weak_m_proportion'] = master_output[i]['total_weak_m_proportion']\n",
    "    temp['total_constraining_proportion'] = master_output[i]['total_constraining_proportion']\n",
    "    \n",
    "    temp['summed_negative_weights'] = master_output[i]['summed_negative_weights']\n",
    "    temp['summed_positive_weights'] = master_output[i]['summed_positive_weights']\n",
    "    temp['summed_uncertain_weights'] = master_output[i]['summed_uncertain_weights']\n",
    "    temp['summed_litigious_weights'] = master_output[i]['summed_litigious_weights']\n",
    "    temp['summed_strong_m_weights'] = master_output[i]['summed_strong_m_weights']\n",
    "    temp['summed_weak_m_weights'] = master_output[i]['summed_weak_m_weights']\n",
    "    temp['summed_constraining_weights'] = master_output[i]['summed_constraining_weights']\n",
    "    final_list.append(temp)\n",
    "    \n",
    "# Final dataframe\n",
    "final_df = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('metastore/bag_of_words_2001-2015.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110a93b-cd55-4923-b2f6-e7380ceb0bba",
   "metadata": {},
   "source": [
    "# Union of 2001-2015 & 2015-2021 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1483c-565f-4807-84b2-8eca9ea08fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_2001_2015 = pd.read_csv('metastore/bag_of_words_2001-2015.csv')\n",
    "new_2015_2021 = pd.read_csv('metastore/bag_of_words_2015-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece2ce0-6fbc-4992-ab8a-ff6722c6fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([old_2001_2015, new_2015_2021])\n",
    "combined_data.to_csv('metastore/bag_of_words_2001-2021.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1b1bc-88db-4eb0-91d2-89a69617f601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
